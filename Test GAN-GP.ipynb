{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import  scipy.signal\n",
    "\n",
    "time_arr =  np.arange(0, 10, 1/16000)\n",
    "\n",
    "\n",
    "signal = np.sin(2*np.pi*200*time_arr)+np.sin(2*np.pi*2500*time_arr)+np.sin(2*np.pi*5500*time_arr)\n",
    "stft = np.abs(scipy.signal.stft(signal, fs=16000.0, window='hann', nperseg=512, noverlap=512-128)[2])\n",
    "print(stft.dtype)\n",
    "plt.imshow(stft)\n",
    "plt.show()\n",
    "\n",
    "fp = np.memmap('data/test_2.dat', dtype='float64', mode='w+', shape=stft.shape)\n",
    "fp[:] = stft[:]\n",
    "del fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.init()\n",
    "from torch import nn\n",
    "\n",
    "from model.borderEncoder import BorderEncoder\n",
    "from model.discriminator import Discriminator\n",
    "from model.generator import Generator\n",
    "\n",
    "import time\n",
    "\n",
    "from utils.colorize import colorize\n",
    "import torch.autograd as autograd\n",
    "\n",
    "\n",
    "def calc_gradient_penalty_bayes(discriminator, real_data, fake_data, gamma):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    batch_size = real_data.size()[0]\n",
    "\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1)\n",
    "    alpha = alpha.expand(real_data.size()).to(device)\n",
    "\n",
    "    interpolates = alpha * real_data + ((1 - alpha) * fake_data)\n",
    "\n",
    "    interpolates = autograd.Variable(interpolates, requires_grad=True).to(device)\n",
    "\n",
    "    disc_interpolates = discriminator(interpolates)\n",
    "\n",
    "    gradients = autograd.grad(outputs=disc_interpolates, inputs=interpolates,\n",
    "                              grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n",
    "                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "#     gradients = gradients.view(real)\n",
    "    print(gradients.norm(2, dim=[1,2,3]).mean())\n",
    "    gradient_penalty = ((gradients.norm(2, dim=[1,2,3]) - 1) ** 2).mean() * gamma\n",
    "\n",
    "    return gradient_penalty\n",
    "\n",
    "\n",
    "__author__ = 'Andres'\n",
    "\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "        m.bias.data.fill_(0.0)\n",
    "\n",
    "\n",
    "def train(args, device, train_loader, epoch, summary_writer):\n",
    "    discriminators = nn.ModuleList(\n",
    "            [Discriminator(args['discriminator'], args['discriminator_in_shape']) for _ in range(3)]\n",
    "        ).to(device)\n",
    "    discriminators.apply(init_weights)\n",
    "\n",
    "    left_border_encoder = BorderEncoder(args['borderEncoder']).to(device)\n",
    "    right_border_encoder = BorderEncoder(args['borderEncoder']).to(device)\n",
    "    left_border_encoder.apply(init_weights)\n",
    "    right_border_encoder.apply(init_weights)\n",
    "\n",
    "    generator = Generator(args['generator'], args['generator_input']).to(device)\n",
    "    generator.apply(init_weights)\n",
    "\n",
    "    optim_g = torch.optim.Adam(list(generator.parameters()) + list(left_border_encoder.parameters()) +\n",
    "                               list(right_border_encoder.parameters()),\n",
    "                               lr=args['optimizer']['generator']['learning_rate'],\n",
    "                               betas=(0.5, 0.9))\n",
    "    optims_d = [torch.optim.Adam(discriminator.parameters(),\n",
    "                               lr=args['optimizer']['discriminator']['learning_rate'],\n",
    "                               betas=(0.5, 0.9)) for discriminator in discriminators]\n",
    "    # try:\n",
    "    # left_border_encoder.train()\n",
    "    # right_border_encoder.train()\n",
    "    # generator.train()\n",
    "    # discriminators.train()\n",
    "    print('try')\n",
    "    start_time = time.time()\n",
    "    prev_iter_time = start_time\n",
    "    # train_loader = tqdm.tqdm(train_loader)\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        print(batch_idx)\n",
    "        data = data.to(device).float()\n",
    "        data = data.view(args['optimizer']['batch_size'], *args['spectrogram_shape'])\n",
    "        real_spectrograms = data[::2]\n",
    "        fake_left_borders = data[1::2, :, :, :args['split'][0]]\n",
    "        fake_right_borders = data[1::2, :, :, args['split'][0] + args['split'][1]:]\n",
    "\n",
    "        #optimize D\n",
    "\n",
    "        for _ in range(args['optimizer']['n_critic']):\n",
    "            for index, (discriminator, optim_d) in enumerate(zip(discriminators, optims_d)):\n",
    "                optim_d.zero_grad()\n",
    "                print(index)\n",
    "                encoded_left_border = left_border_encoder(fake_left_borders)\n",
    "                encoded_right_border = right_border_encoder(fake_right_borders)\n",
    "                encoded_size = encoded_left_border.size()\n",
    "                noise = torch.rand(encoded_size[0], 4, encoded_size[2], encoded_size[3]).to(device)\n",
    "                generated_spectrograms = generator(torch.cat((encoded_left_border, encoded_right_border, noise), 1))\n",
    "\n",
    "                fake_spectrograms = torch.cat((fake_left_borders, generated_spectrograms, fake_right_borders), 3)\n",
    "                scale = 2**index\n",
    "                time_axis = args['spectrogram_shape'][2]\n",
    "                start = int((time_axis - (time_axis // 4) * scale) / 2)\n",
    "                end = time_axis - start\n",
    "                x_fake = fake_spectrograms[:, :, :, start:end:scale]\n",
    "                x_real = real_spectrograms[:, :, :, start:end:scale]\n",
    "\n",
    "                d_loss_r = torch.mean(discriminator(x_real))\n",
    "                d_loss_f = torch.mean(discriminator(x_fake))\n",
    "\n",
    "                grad_pen = calc_gradient_penalty_bayes(discriminator, x_real, x_fake, args['gamma_gp'])\n",
    "                d_loss_gp = torch.mean(grad_pen)\n",
    "                disc_loss = d_loss_f - d_loss_r + d_loss_gp\n",
    "                print(d_loss_gp)\n",
    "\n",
    "                disc_loss.backward()\n",
    "                optim_d.step()\n",
    "\n",
    "#             grad_pen_2 = wgan_regularization(discriminators, x_real, x_fake, args['gamma_gp'])\n",
    "#             d_loss_gp_2 += grad_pen_2.mean()\n",
    "#             grad_pen_3 = calc_gradient_penalty(discriminators, x_real, x_fake, args['gamma_gp'])\n",
    "#             d_loss_gp_3 += grad_pen_3.mean()\n",
    "\n",
    "#             print(d_loss_gp_2)\n",
    "#             print(d_loss_gp_3)\n",
    "\n",
    "        #optimize G\n",
    "\n",
    "        optim_g.zero_grad()\n",
    "\n",
    "        encoded_left_border = left_border_encoder(fake_left_borders)\n",
    "        encoded_right_border = right_border_encoder(fake_right_borders)\n",
    "        encoded_size = encoded_left_border.size()\n",
    "        noise = torch.rand(encoded_size[0], 4, encoded_size[2], encoded_size[3]).to(device)\n",
    "        generated_spectrograms = generator(torch.cat((encoded_left_border, encoded_right_border, noise), 1))\n",
    "\n",
    "        fake_spectrograms = torch.cat((fake_left_borders, generated_spectrograms, fake_right_borders), 3)\n",
    "        d_loss_f = 0\n",
    "\n",
    "        for index, discriminator in enumerate(discriminators):\n",
    "            scale = 2 ** index\n",
    "            time_axis = args['spectrogram_shape'][2]\n",
    "            start = int((time_axis - (time_axis // 4) * scale) / 2)\n",
    "            end = time_axis - start\n",
    "            x_fake = fake_spectrograms[:, :, :, start:end:scale]\n",
    "            d_loss_f += torch.mean(discriminator(x_fake))\n",
    "\n",
    "        gen_loss = - d_loss_f\n",
    "        gen_loss.backward()\n",
    "        optim_g.step()\n",
    "\n",
    "#         if batch_idx % args['log_interval'] == 0:\n",
    "#             current_time = time.time()\n",
    "\n",
    "#             print(\" * Epoch: [{:2d}] [{:4d}/{:4d} ({:.0f}%)] \"\n",
    "#                   \"Counter:{:2d}\\t\"\n",
    "#                   \"({:4.1f} min\\t\"\n",
    "#                   \"{:4.3f} examples/sec\\t\"\n",
    "#                   \"{:4.2f} sec/batch)\\n\"\n",
    "#                   \"   Disc batch loss:{:.8f}\\t\"\n",
    "#                   \"   Gen batch loss:{:.8f}\\t\"\n",
    "#                   \"   Reg batch :{:.8f}\\t\".format(\n",
    "#                 int(epoch),\n",
    "#                 int(batch_idx*len(data)),\n",
    "#                 int(len(train_loader.dataset)/len(data)), 100. * batch_idx / len(train_loader), int(batch_idx),\n",
    "#                 (current_time - start_time) / 60,\n",
    "#                 args['log_interval'] * args['optimizer']['batch_size'] / (current_time - prev_iter_time),\n",
    "#                 (current_time - prev_iter_time) / args['log_interval'],\n",
    "#                 disc_loss.item(),\n",
    "#                 gen_loss.item(),\n",
    "#                 d_loss_gp.item()))\n",
    "#             prev_iter_time = current_time\n",
    "#         if batch_idx % args['tensorboard_interval'] == 0:\n",
    "#             summary_writer.add_scalar(\"Disc/Neg_Loss\", -disc_loss, global_step=batch_idx)\n",
    "#             summary_writer.add_scalar(\"Disc/Neg_Critic\", d_loss_f - d_loss_r, global_step=batch_idx)\n",
    "#             summary_writer.add_scalar(\"Disc/Loss_f\", d_loss_f, global_step=batch_idx)\n",
    "#             summary_writer.add_scalar(\"Disc/Loss_r\", d_loss_r, global_step=batch_idx)\n",
    "#             summary_writer.add_scalar(\"Gen/Loss\", gen_loss, global_step=batch_idx)\n",
    "\n",
    "#             for index in range(4):\n",
    "#                 summary_writer.add_image(\"images/Real_Image/\" + str(index), colorize(real_spectrograms[index]), global_step=batch_idx)\n",
    "#                 summary_writer.add_image(\"images/Fake_Image/\" + str(index), colorize(fake_spectrograms[index], -1, 1), global_step=batch_idx)\n",
    "#                 # except Exception as e:\n",
    "            # print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data.trainDataset import TrainDataset\n",
    "import logging\n",
    "\n",
    "# logging.getLogger().setLevel(logging.DEBUG)  # set root logger to debug\n",
    "\n",
    "\"\"\"Just so logging works...\"\"\"\n",
    "formatter = logging.Formatter('%(name)s:%(levelname)s:%(message)s')\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "console_handler.setFormatter(formatter)\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "\"\"\"Just so logging works...\"\"\"\n",
    "\n",
    "__author__ = 'Andres'\n",
    "\n",
    "\n",
    "signal_split = [192, 128, 192]\n",
    "md = 4\n",
    "\n",
    "params_discriminator = dict()\n",
    "params_discriminator['stride'] = [2,2,2,2,2]\n",
    "params_discriminator['nfilter'] = [md, 2*md, 4*md, 8*md, 16*md]\n",
    "params_discriminator['shape'] = [[5, 5], [5, 5], [5, 5], [5, 5], [5, 5]]\n",
    "params_discriminator['full'] = []\n",
    "params_discriminator['minibatch_reg'] = False\n",
    "params_discriminator['summary'] = True\n",
    "params_discriminator['data_size'] = 2\n",
    "params_discriminator['apply_phaseshuffle'] = True\n",
    "params_discriminator['spectral_norm'] = True\n",
    "\n",
    "params_generator = dict()\n",
    "params_generator['stride'] = [2, 2, 2, 2, 2]\n",
    "params_generator['latent_dim'] = 100\n",
    "params_generator['nfilter'] = [8*md, 4*md, 2*md, md, 1]\n",
    "params_generator['shape'] = [[4, 4], [4, 4], [8, 8], [8, 8], [8, 8]]\n",
    "params_generator['padding'] = [[1, 1], [1, 1], [3, 3], [3, 3], [3, 3]]\n",
    "params_generator['full'] = 256*md\n",
    "params_generator['summary'] = True\n",
    "params_generator['data_size'] = 2\n",
    "params_generator['spectral_norm'] = True\n",
    "params_generator['in_conv_shape'] = [8, 4]\n",
    "params_generator['borders'] = dict()\n",
    "params_generator['borders']['nfilter'] = [md, 2*md, md, md/2]\n",
    "params_generator['borders']['shape'] = [[5, 5],[5, 5],[5, 5],[5, 5]]\n",
    "params_generator['borders']['stride'] = [2, 2, 3, 4]\n",
    "params_generator['borders']['data_size'] = 2\n",
    "# This does not work because of flipping, border 2 need to be flipped tf.reverse(l, axis=[1]), ask Nathanael\n",
    "params_generator['borders']['width_full'] = None\n",
    "\n",
    "\n",
    "# Optimization parameters inspired from 'Self-Attention Generative Adversarial Networks'\n",
    "# - Spectral normalization GEN DISC\n",
    "# - Batch norm GEN\n",
    "# - TTUR ('GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium')\n",
    "# - ADAM  beta1=0 beta2=0.9, disc lr 0.0004, gen lr 0.0001\n",
    "# - Hinge loss\n",
    "# Parameters are similar to the ones in those papers...\n",
    "# - 'PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION'\n",
    "# - 'LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS'\n",
    "# - 'CGANS WITH PROJECTION DISCRIMINATOR'\n",
    "\n",
    "params_optimization = dict()\n",
    "params_optimization['batch_size'] = 64*2\n",
    "params_discriminator['batch_size'] = 64*2\n",
    "\n",
    "params_optimization['epoch'] = 600\n",
    "params_optimization['n_critic'] = 2\n",
    "params_optimization['generator'] = dict()\n",
    "params_optimization['generator']['optimizer'] = 'adam'\n",
    "params_optimization['generator']['kwargs'] = [0.5, 0.9]\n",
    "params_optimization['generator']['learning_rate'] = 1e-4\n",
    "params_optimization['discriminator'] = dict()\n",
    "params_optimization['discriminator']['optimizer'] = 'adam'\n",
    "params_optimization['discriminator']['kwargs'] = [0.5, 0.9]\n",
    "params_optimization['discriminator']['learning_rate'] = 1e-4\n",
    "\n",
    "# all parameters\n",
    "params = dict()\n",
    "params['net'] = dict() # All the parameters for the model\n",
    "params['net']['generator'] = params_generator\n",
    "params['net']['discriminator'] = params_discriminator\n",
    "params['net']['prior_distribution'] = 'gaussian'\n",
    "params['net']['shape'] = [1, 256, 128*4] # Shape of the image\n",
    "params['net']['inpainting']=dict()\n",
    "params['net']['inpainting']['split']=signal_split\n",
    "params['net']['gamma_gp'] = 10 # Gradient penalty\n",
    "# params['net']['fs'] = 16000//downscale\n",
    "params['net']['loss_type'] ='wasserstein'\n",
    "\n",
    "params['optimization'] = params_optimization\n",
    "params['summary_every'] = 250 # Tensorboard summaries every ** iterations\n",
    "params['print_every'] = 50 # Console summaries every ** iterations\n",
    "params['save_every'] = 1000 # Save the model every ** iterations\n",
    "# params['summary_dir'] = os.path.join(global_path, name +'_summary/')\n",
    "# params['save_dir'] = os.path.join(global_path, name + '_checkpoints/')\n",
    "params['Nstats'] = 500\n",
    "\n",
    "args = dict()\n",
    "args['generator'] = params_generator\n",
    "args['discriminator'] = params_discriminator\n",
    "args['borderEncoder'] = params_generator['borders']\n",
    "args['discriminator_in_shape'] = [1, 256, 128]\n",
    "args['generator_input'] = 2*6*4*2+24*4\n",
    "args['optimizer'] = params_optimization\n",
    "args['split'] = signal_split\n",
    "args['log_interval'] = 50\n",
    "args['spectrogram_shape'] = params['net']['shape']\n",
    "args['gamma_gp'] = params['net']['gamma_gp']\n",
    "args['tensorboard_interval'] = 250\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "examples_per_file = 16\n",
    "trainDataset = TrainDataset(\"data/\", window_size=512, examples_per_file=examples_per_file)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainDataset,\n",
    "    batch_size=args['optimizer']['batch_size']//examples_per_file, shuffle=True)#,\n",
    "                                           #num_workers=1, drop_last=True)\n",
    "\n",
    "\n",
    "experiment_name = 'pytorch'\n",
    "# summary_writer = SummaryWriter('saved_results/' + experiment_name)\n",
    "\n",
    "# for epoch in range(10):\n",
    "train(args, device, train_loader, epoch, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
