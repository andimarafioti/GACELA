{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "import torch\n",
    "\n",
    "from data.audioLoader import AudioLoader\n",
    "from data.trainDataset import TrainDataset\n",
    "from ganSystem import GANSystem\n",
    "import logging\n",
    "\n",
    "# logging.getLogger().setLevel(logging.DEBUG)  # set root logger to debug\n",
    "\n",
    "\"\"\"Just so logging works...\"\"\"\n",
    "formatter = logging.Formatter('%(name)s:%(levelname)s:%(message)s')\n",
    "console_handler = logging.StreamHandler()\n",
    "console_handler.setLevel(logging.DEBUG)\n",
    "console_handler.setFormatter(formatter)\n",
    "logging.getLogger().addHandler(console_handler)\n",
    "\"\"\"Just so logging works...\"\"\"\n",
    "\n",
    "__author__ = 'Andres'\n",
    "\n",
    "signal_split = [480, 64, 480]\n",
    "md = 32\n",
    "\n",
    "params_stft_discriminator = dict()\n",
    "params_stft_discriminator['stride'] = [2, 2, 2, 2, 2]\n",
    "params_stft_discriminator['nfilter'] = [md, 2 * md, 4 * md, 8 * md, 16 * md]\n",
    "params_stft_discriminator['shape'] = [[5, 5], [5, 5], [5, 5], [5, 5], [5, 5]]\n",
    "params_stft_discriminator['data_size'] = 2\n",
    "\n",
    "params_mel_discriminator = dict()\n",
    "params_mel_discriminator['stride'] = [2, 2, 2, 2, 2]\n",
    "params_mel_discriminator['nfilter'] = [md//4, 2 * md//4, 4 * md//4, 8 * md//4, 16 * md//4]\n",
    "params_mel_discriminator['shape'] = [[5, 5], [5, 5], [5, 5], [5, 5], [5, 5]]\n",
    "params_mel_discriminator['data_size'] = 2\n",
    "\n",
    "params_generator = dict()\n",
    "params_generator['stride'] = [2, 2, 2, 2, 2]\n",
    "params_generator['nfilter'] = [8 * md, 4 * md, 2 * md, md, 1]\n",
    "params_generator['shape'] = [[4, 4], [4, 4], [8, 8], [8, 8], [8, 8]]\n",
    "params_generator['padding'] = [[1, 1], [1, 1], [3, 3], [3, 3], [3, 3]]\n",
    "params_generator['residual_blocks'] = 2\n",
    "\n",
    "params_generator['full'] = 256 * md\n",
    "params_generator['summary'] = True\n",
    "params_generator['data_size'] = 2\n",
    "params_generator['in_conv_shape'] = [16, 2]\n",
    "params_generator['borders'] = dict()\n",
    "params_generator['borders']['nfilter'] = [md, 2 * md, md, md / 2]\n",
    "params_generator['borders']['shape'] = [[5, 5], [5, 5], [5, 5], [5, 5]]\n",
    "params_generator['borders']['stride'] = [2, 2, 2, 2]\n",
    "params_generator['borders']['data_size'] = 2\n",
    "params_generator['borders']['border_scale'] = 1\n",
    "# This does not work because of flipping, border 2 need to be flipped tf.reverse(l, axis=[1]), ask Nathanael\n",
    "params_generator['borders']['width_full'] = None\n",
    "\n",
    "# Optimization parameters inspired from 'Self-Attention Generative Adversarial Networks'\n",
    "# - Spectral normalization GEN DISC\n",
    "# - Batch norm GEN\n",
    "# - TTUR ('GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium')\n",
    "# - ADAM  beta1=0 beta2=0.9, disc lr 0.0004, gen lr 0.0001\n",
    "# - Hinge loss\n",
    "# Parameters are similar to the ones in those papers...\n",
    "# - 'PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION'\n",
    "# - 'LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS'\n",
    "# - 'CGANS WITH PROJECTION DISCRIMINATOR'\n",
    "\n",
    "params_optimization = dict()\n",
    "params_optimization['batch_size'] = 64\n",
    "params_stft_discriminator['batch_size'] = 64\n",
    "params_mel_discriminator['batch_size'] = 64\n",
    "\n",
    "params_optimization['n_critic'] = 1\n",
    "params_optimization['generator'] = dict()\n",
    "params_optimization['generator']['optimizer'] = 'adam'\n",
    "params_optimization['generator']['kwargs'] = [0.5, 0.9]\n",
    "params_optimization['generator']['learning_rate'] = 1e-4\n",
    "params_optimization['discriminator'] = dict()\n",
    "params_optimization['discriminator']['optimizer'] = 'adam'\n",
    "params_optimization['discriminator']['kwargs'] = [0.5, 0.9]\n",
    "params_optimization['discriminator']['learning_rate'] = 1e-4\n",
    "\n",
    "# all parameters\n",
    "params = dict()\n",
    "params['net'] = dict()  # All the parameters for the model\n",
    "params['net']['generator'] = params_generator\n",
    "params['net']['stft_discriminator'] = params_stft_discriminator\n",
    "params['net']['mel_discriminator'] = params_mel_discriminator\n",
    "params['net']['prior_distribution'] = 'gaussian'\n",
    "params['net']['shape'] = [1, 512, 1024]  # Shape of the image\n",
    "params['net']['inpainting'] = dict()\n",
    "params['net']['inpainting']['split'] = signal_split\n",
    "params['net']['gamma_gp'] = 10  # Gradient penalty\n",
    "# params['net']['fs'] = 16000//downscale\n",
    "params['net']['loss_type'] = 'wasserstein'\n",
    "\n",
    "params['optimization'] = params_optimization\n",
    "params['summary_every'] = 250  # Tensorboard summaries every ** iterations\n",
    "params['print_every'] = 50  # Console summaries every ** iterations\n",
    "params['save_every'] = 1000  # Save the model every ** iterations\n",
    "# params['summary_dir'] = os.path.join(global_path, name +'_summary/')\n",
    "# params['save_dir'] = os.path.join(global_path, name + '_checkpoints/')\n",
    "\n",
    "args = dict()\n",
    "args['generator'] = params_generator\n",
    "args['stft_discriminator_count'] = 2\n",
    "args['mel_discriminator_count'] = 3\n",
    "args['stft_discriminator'] = params_stft_discriminator\n",
    "args['mel_discriminator'] = params_mel_discriminator\n",
    "args['borderEncoder'] = params_generator['borders']\n",
    "args['stft_discriminator_in_shape'] = [1, 512, 64]\n",
    "args['mel_discriminator_in_shape'] = [1, 80, 64]\n",
    "args['mel_discriminator_start_powscale'] = 2\n",
    "args['generator_input'] = 1440\n",
    "args['optimizer'] = params_optimization\n",
    "args['split'] = signal_split\n",
    "args['log_interval'] = 100\n",
    "args['spectrogram_shape'] = params['net']['shape']\n",
    "args['gamma_gp'] = params['net']['gamma_gp']\n",
    "args['tensorboard_interval'] = 500\n",
    "args['save_path'] = '../saved_results/'\n",
    "args['experiment_name'] = 'real_data'\n",
    "args['save_interval'] = 10000\n",
    "\n",
    "args['fft_length'] = 1024\n",
    "args['fft_hop_size'] = 256\n",
    "args['sampling_rate'] = 22050\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "examples_per_file = 32\n",
    "audioLoader = AudioLoader(args['sampling_rate'], args['fft_length'], args['fft_hop_size'], 50)\n",
    "\n",
    "dataFolder = \"../../../../Datasets/maestro-v2.0.0/\"\n",
    "\n",
    "ganSystem = GANSystem(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_at_step = 400000\n",
    "start_at_epoch = 1\n",
    "\n",
    "ganSystem.loadModel(start_at_step, start_at_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.audioLoader import AudioLoader\n",
    "from data.validDataset import ValidDataset\n",
    "\n",
    "examples_per_file=1\n",
    "validDataset = ValidDataset(dataFolder, window_size=2048, audio_loader=audioLoader, examples_per_file=examples_per_file, loaded_files_buffer=20, file_usages=1)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(validDataset,\n",
    "    batch_size=args['optimizer']['batch_size']//examples_per_file, shuffle=False,\n",
    "                                           num_workers=0, drop_last=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for data in valid_loader:\n",
    "    spectrograms=data[0]\n",
    "    audios = data[1].view(args['optimizer']['batch_size'], -1)\n",
    "    \n",
    "    spectrograms = spectrograms.to(device).float()\n",
    "    spectrograms = spectrograms.view(args['optimizer']['batch_size'], *[1, 512, 2048])\n",
    "    \n",
    "    left_borders = spectrograms[:, :, :, 512:512+args['split'][0]]\n",
    "    right_borders = spectrograms[:, :, :, 512+args['split'][0] + args['split'][1]:-512]\n",
    "    print('generate')\n",
    "    generated_spectrograms = ganSystem.generateGap([left_borders, right_borders])\n",
    "\n",
    "    fake_spectrograms = torch.cat((left_borders, generated_spectrograms, right_borders), 3)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios = data[1].view(args['optimizer']['batch_size'], -1)\n",
    "spectrograms = spectrograms.view(args['optimizer']['batch_size'], *[1, 512, 2048])\n",
    "\n",
    "left_borders = spectrograms[:, :, :, 512:512+args['split'][0]]\n",
    "right_borders = spectrograms[:, :, :, 512+args['split'][0] + args['split'][1]:-512]\n",
    "print('generate')\n",
    "generated_spectrograms = ganSystem.generateGap([left_borders, right_borders])\n",
    "\n",
    "fake_spectrograms = torch.cat((left_borders, generated_spectrograms, right_borders), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifresi.stft import GaussTruncTF, GaussTF\n",
    "from tifresi.transforms import inv_log_spectrogram, log_spectrogram\n",
    "import numpy as np\n",
    "\n",
    "stft = GaussTruncTF(256, 1024)\n",
    "fake_audios = np.zeros([len(audios), fake_spectrograms.shape[-1]*args['fft_hop_size']])\n",
    "\n",
    "for index, (real_audio, spectrogram) in enumerate(zip(audios, fake_spectrograms)):\n",
    "    unprocessed_spectrogram = inv_log_spectrogram((spectrogram-1)*25).squeeze().detach().cpu()\n",
    "    unprocessed_spectrogram = np.concatenate([unprocessed_spectrogram,\n",
    "                                  np.zeros_like(unprocessed_spectrogram)[0:1, :]], axis=0) # Fill last column of freqs with zeros    \n",
    "    audio = stft.invert_spectrogram(unprocessed_spectrogram)\n",
    "    fake_audios[index] = audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_audios = audios.detach().cpu().numpy().copy()    \n",
    "fake_medium_audios = fake_audios.copy()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_split = [240, 32, 240]\n",
    "md = 32\n",
    "\n",
    "params_stft_discriminator = dict()\n",
    "params_stft_discriminator['stride'] = [2, 2, 2, 2, 2]\n",
    "params_stft_discriminator['nfilter'] = [md, 2 * md, 4 * md, 8 * md, 16 * md]\n",
    "params_stft_discriminator['shape'] = [[5, 5], [5, 5], [5, 5], [5, 5], [5, 5]]\n",
    "params_stft_discriminator['data_size'] = 2\n",
    "\n",
    "params_mel_discriminator = dict()\n",
    "params_mel_discriminator['stride'] = [2, 2, 2, 2, 2]\n",
    "params_mel_discriminator['nfilter'] = [md//4, 2 * md//4, 4 * md//4, 8 * md//4, 16 * md//4]\n",
    "params_mel_discriminator['shape'] = [[5, 5], [5, 5], [5, 5], [5, 5], [5, 5]]\n",
    "params_mel_discriminator['data_size'] = 2\n",
    "\n",
    "params_generator = dict()\n",
    "params_generator['stride'] = [2, 2, 2, 2, 2]\n",
    "params_generator['nfilter'] = [8 * md, 4 * md, 2 * md, md, 1]\n",
    "params_generator['shape'] = [[4, 4], [4, 4], [8, 8], [8, 8], [8, 8]]\n",
    "params_generator['padding'] = [[1, 1], [1, 1], [3, 3], [3, 3], [3, 3]]\n",
    "params_generator['residual_blocks'] = 2\n",
    "\n",
    "params_generator['full'] = 256 * md\n",
    "params_generator['summary'] = True\n",
    "params_generator['data_size'] = 2\n",
    "params_generator['in_conv_shape'] = [16, 1]\n",
    "params_generator['borders'] = dict()\n",
    "params_generator['borders']['nfilter'] = [md, 2 * md, md, md / 2]\n",
    "params_generator['borders']['shape'] = [[5, 5], [5, 5], [5, 5], [5, 5]]\n",
    "params_generator['borders']['stride'] = [2, 2, 2, 2]\n",
    "params_generator['borders']['data_size'] = 2\n",
    "params_generator['borders']['border_scale'] = 1\n",
    "# This does not work because of flipping, border 2 need to be flipped tf.reverse(l, axis=[1]), ask Nathanael\n",
    "params_generator['borders']['width_full'] = None\n",
    "\n",
    "# Optimization parameters inspired from 'Self-Attention Generative Adversarial Networks'\n",
    "# - Spectral normalization GEN DISC\n",
    "# - Batch norm GEN\n",
    "# - TTUR ('GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium')\n",
    "# - ADAM  beta1=0 beta2=0.9, disc lr 0.0004, gen lr 0.0001\n",
    "# - Hinge loss\n",
    "# Parameters are similar to the ones in those papers...\n",
    "# - 'PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION'\n",
    "# - 'LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS'\n",
    "# - 'CGANS WITH PROJECTION DISCRIMINATOR'\n",
    "\n",
    "params_optimization = dict()\n",
    "params_optimization['batch_size'] = 64\n",
    "params_stft_discriminator['batch_size'] = 64\n",
    "params_mel_discriminator['batch_size'] = 64\n",
    "\n",
    "params_optimization['n_critic'] = 1\n",
    "params_optimization['generator'] = dict()\n",
    "params_optimization['generator']['optimizer'] = 'adam'\n",
    "params_optimization['generator']['kwargs'] = [0.5, 0.9]\n",
    "params_optimization['generator']['learning_rate'] = 1e-4\n",
    "params_optimization['discriminator'] = dict()\n",
    "params_optimization['discriminator']['optimizer'] = 'adam'\n",
    "params_optimization['discriminator']['kwargs'] = [0.5, 0.9]\n",
    "params_optimization['discriminator']['learning_rate'] = 1e-4\n",
    "\n",
    "# all parameters\n",
    "params = dict()\n",
    "params['net'] = dict()  # All the parameters for the model\n",
    "params['net']['generator'] = params_generator\n",
    "params['net']['stft_discriminator'] = params_stft_discriminator\n",
    "params['net']['mel_discriminator'] = params_mel_discriminator\n",
    "params['net']['prior_distribution'] = 'gaussian'\n",
    "params['net']['shape'] = [1, 512, 512]  # Shape of the image\n",
    "params['net']['inpainting'] = dict()\n",
    "params['net']['inpainting']['split'] = signal_split\n",
    "params['net']['gamma_gp'] = 10  # Gradient penalty\n",
    "# params['net']['fs'] = 16000//downscale\n",
    "params['net']['loss_type'] = 'wasserstein'\n",
    "\n",
    "params['optimization'] = params_optimization\n",
    "params['summary_every'] = 250  # Tensorboard summaries every ** iterations\n",
    "params['print_every'] = 50  # Console summaries every ** iterations\n",
    "params['save_every'] = 1000  # Save the model every ** iterations\n",
    "# params['summary_dir'] = os.path.join(global_path, name +'_summary/')\n",
    "# params['save_dir'] = os.path.join(global_path, name + '_checkpoints/')\n",
    "\n",
    "args = dict()\n",
    "args['generator'] = params_generator\n",
    "args['stft_discriminator_count'] = 2\n",
    "args['mel_discriminator_count'] = 3\n",
    "args['stft_discriminator'] = params_stft_discriminator\n",
    "args['mel_discriminator'] = params_mel_discriminator\n",
    "args['borderEncoder'] = params_generator['borders']\n",
    "args['stft_discriminator_in_shape'] = [1, 512, 32]\n",
    "args['mel_discriminator_in_shape'] = [1, 80, 32]\n",
    "args['mel_discriminator_start_powscale'] = 2\n",
    "args['generator_input'] = 720\n",
    "args['optimizer'] = params_optimization\n",
    "args['split'] = signal_split\n",
    "args['log_interval'] = 100\n",
    "args['spectrogram_shape'] = params['net']['shape']\n",
    "args['gamma_gp'] = params['net']['gamma_gp']\n",
    "args['tensorboard_interval'] = 500\n",
    "args['save_path'] = '../saved_results/'\n",
    "args['experiment_name'] = 'real_data_240_32_240'\n",
    "args['save_interval'] = 10000\n",
    "\n",
    "args['fft_length'] = 1024\n",
    "args['fft_hop_size'] = 256\n",
    "args['sampling_rate'] = 22050\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "examples_per_file = 32\n",
    "audioLoader = AudioLoader(args['sampling_rate'], args['fft_length'], args['fft_hop_size'], 50)\n",
    "\n",
    "ganSystem = GANSystem(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_at_step = 310000\n",
    "start_at_epoch = 1\n",
    "\n",
    "ganSystem.loadModel(start_at_step, start_at_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_borders = spectrograms[:, :, :, 512+256:512+256+args['split'][0]]\n",
    "right_borders = spectrograms[:, :, :, 512+256+args['split'][0] + args['split'][1]:-512-256]\n",
    "print('generate')\n",
    "generated_spectrograms = ganSystem.generateGap([left_borders, right_borders])\n",
    "\n",
    "fake_spectrograms = torch.cat((left_borders, generated_spectrograms, right_borders), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifresi.stft import GaussTruncTF, GaussTF\n",
    "from tifresi.transforms import inv_log_spectrogram, log_spectrogram\n",
    "import numpy as np\n",
    "\n",
    "stft = GaussTruncTF(256, 1024)\n",
    "fake_audios = np.zeros([len(audios), fake_spectrograms.shape[-1]*args['fft_hop_size']])\n",
    "\n",
    "for index, (real_audio, spectrogram) in enumerate(zip(audios, fake_spectrograms)):\n",
    "    unprocessed_spectrogram = inv_log_spectrogram((spectrogram-1)*25).squeeze().detach().cpu()\n",
    "    unprocessed_spectrogram = np.concatenate([unprocessed_spectrogram,\n",
    "                                  np.zeros_like(unprocessed_spectrogram)[0:1, :]], axis=0) # Fill last column of freqs with zeros    \n",
    "    audio = stft.invert_spectrogram(unprocessed_spectrogram)\n",
    "    fake_audios[index] = audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Audio\n",
    "\n",
    "for generated_audio_signal in fake_audios:\n",
    "    display(Audio(generated_audio_signal[int(len(generated_audio_signal)*0.3):], rate=22050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_short_audios = fake_audios.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_split = [960, 128, 960]\n",
    "md = 32\n",
    "\n",
    "params_stft_discriminator = dict()\n",
    "params_stft_discriminator['stride'] = [2, 2, 2, 2, 2]\n",
    "params_stft_discriminator['nfilter'] = [md, 2 * md, 4 * md, 8 * md, 16 * md]\n",
    "params_stft_discriminator['shape'] = [[5, 5], [5, 5], [5, 5], [5, 5], [5, 5]]\n",
    "params_stft_discriminator['data_size'] = 2\n",
    "\n",
    "params_mel_discriminator = dict()\n",
    "params_mel_discriminator['stride'] = [2, 2, 2, 2, 2]\n",
    "params_mel_discriminator['nfilter'] = [md//4, 2 * md//4, 4 * md//4, 8 * md//4, 16 * md//4]\n",
    "params_mel_discriminator['shape'] = [[5, 5], [5, 5], [5, 5], [5, 5], [5, 5]]\n",
    "params_mel_discriminator['data_size'] = 2\n",
    "\n",
    "params_generator = dict()\n",
    "params_generator['stride'] = [2, 2, 2, 2, 2]\n",
    "params_generator['nfilter'] = [8 * md, 4 * md, 2 * md, md, 1]\n",
    "params_generator['shape'] = [[4, 4], [4, 4], [8, 8], [8, 8], [8, 8]]\n",
    "params_generator['padding'] = [[1, 1], [1, 1], [3, 3], [3, 3], [3, 3]]\n",
    "params_generator['residual_blocks'] = 2\n",
    "\n",
    "params_generator['full'] = 256 * md\n",
    "params_generator['summary'] = True\n",
    "params_generator['data_size'] = 2\n",
    "params_generator['in_conv_shape'] = [16, 4]\n",
    "params_generator['borders'] = dict()\n",
    "params_generator['borders']['nfilter'] = [md, 2 * md, md, md / 2]\n",
    "params_generator['borders']['shape'] = [[5, 5], [5, 5], [5, 5], [5, 5]]\n",
    "params_generator['borders']['stride'] = [2, 2, 2, 2]\n",
    "params_generator['borders']['data_size'] = 2\n",
    "params_generator['borders']['border_scale'] = 1\n",
    "# This does not work because of flipping, border 2 need to be flipped tf.reverse(l, axis=[1]), ask Nathanael\n",
    "params_generator['borders']['width_full'] = None\n",
    "\n",
    "# Optimization parameters inspired from 'Self-Attention Generative Adversarial Networks'\n",
    "# - Spectral normalization GEN DISC\n",
    "# - Batch norm GEN\n",
    "# - TTUR ('GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium')\n",
    "# - ADAM  beta1=0 beta2=0.9, disc lr 0.0004, gen lr 0.0001\n",
    "# - Hinge loss\n",
    "# Parameters are similar to the ones in those papers...\n",
    "# - 'PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION'\n",
    "# - 'LARGE SCALE GAN TRAINING FOR HIGH FIDELITY NATURAL IMAGE SYNTHESIS'\n",
    "# - 'CGANS WITH PROJECTION DISCRIMINATOR'\n",
    "\n",
    "params_optimization = dict()\n",
    "params_optimization['batch_size'] = 64\n",
    "params_stft_discriminator['batch_size'] = 64\n",
    "params_mel_discriminator['batch_size'] = 64\n",
    "\n",
    "params_optimization['n_critic'] = 1\n",
    "params_optimization['generator'] = dict()\n",
    "params_optimization['generator']['optimizer'] = 'adam'\n",
    "params_optimization['generator']['kwargs'] = [0.5, 0.9]\n",
    "params_optimization['generator']['learning_rate'] = 1e-4\n",
    "params_optimization['discriminator'] = dict()\n",
    "params_optimization['discriminator']['optimizer'] = 'adam'\n",
    "params_optimization['discriminator']['kwargs'] = [0.5, 0.9]\n",
    "params_optimization['discriminator']['learning_rate'] = 1e-4\n",
    "\n",
    "# all parameters\n",
    "params = dict()\n",
    "params['net'] = dict()  # All the parameters for the model\n",
    "params['net']['generator'] = params_generator\n",
    "params['net']['stft_discriminator'] = params_stft_discriminator\n",
    "params['net']['mel_discriminator'] = params_mel_discriminator\n",
    "params['net']['prior_distribution'] = 'gaussian'\n",
    "params['net']['shape'] = [1, 512, 2048]  # Shape of the image\n",
    "params['net']['inpainting'] = dict()\n",
    "params['net']['inpainting']['split'] = signal_split\n",
    "params['net']['gamma_gp'] = 10  # Gradient penalty\n",
    "# params['net']['fs'] = 16000//downscale\n",
    "params['net']['loss_type'] = 'wasserstein'\n",
    "\n",
    "params['optimization'] = params_optimization\n",
    "params['summary_every'] = 250  # Tensorboard summaries every ** iterations\n",
    "params['print_every'] = 50  # Console summaries every ** iterations\n",
    "params['save_every'] = 1000  # Save the model every ** iterations\n",
    "# params['summary_dir'] = os.path.join(global_path, name +'_summary/')\n",
    "# params['save_dir'] = os.path.join(global_path, name + '_checkpoints/')\n",
    "\n",
    "args = dict()\n",
    "args['generator'] = params_generator\n",
    "args['stft_discriminator_count'] = 2\n",
    "args['mel_discriminator_count'] = 3\n",
    "args['stft_discriminator'] = params_stft_discriminator\n",
    "args['mel_discriminator'] = params_mel_discriminator\n",
    "args['borderEncoder'] = params_generator['borders']\n",
    "args['stft_discriminator_in_shape'] = [1, 512, 128]\n",
    "args['mel_discriminator_in_shape'] = [1, 80, 128]\n",
    "args['mel_discriminator_start_powscale'] = 2\n",
    "args['generator_input'] = 2700\n",
    "args['optimizer'] = params_optimization\n",
    "args['split'] = signal_split\n",
    "args['log_interval'] = 100\n",
    "args['spectrogram_shape'] = params['net']['shape']\n",
    "args['gamma_gp'] = params['net']['gamma_gp']\n",
    "args['tensorboard_interval'] = 500\n",
    "args['save_path'] = '../saved_results/'\n",
    "args['experiment_name'] = 'real_data_960_128_960'\n",
    "args['save_interval'] = 10000\n",
    "\n",
    "args['fft_length'] = 1024\n",
    "args['fft_hop_size'] = 256\n",
    "args['sampling_rate'] = 22050\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ganSystem = GANSystem(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_at_step = 400000\n",
    "start_at_epoch = 1\n",
    "\n",
    "ganSystem.loadModel(start_at_step, start_at_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left_borders = spectrograms[:, :, :, :args['split'][0]]\n",
    "right_borders = spectrograms[:, :, :, args['split'][0] + args['split'][1]:]\n",
    "print('generate')\n",
    "generated_spectrograms = ganSystem.generateGap([left_borders, right_borders])\n",
    "\n",
    "fake_spectrograms = torch.cat((left_borders, generated_spectrograms, right_borders), 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tifresi.stft import GaussTruncTF, GaussTF\n",
    "from tifresi.transforms import inv_log_spectrogram, log_spectrogram\n",
    "import numpy as np\n",
    "\n",
    "stft = GaussTruncTF(256, 1024)\n",
    "fake_audios = np.zeros([len(audios), fake_spectrograms.shape[-1]*args['fft_hop_size']])\n",
    "\n",
    "for index, (real_audio, spectrogram) in enumerate(zip(audios, fake_spectrograms)):\n",
    "    unprocessed_spectrogram = inv_log_spectrogram((spectrogram-1)*25).squeeze().detach().cpu()\n",
    "    unprocessed_spectrogram = np.concatenate([unprocessed_spectrogram,\n",
    "                                  np.zeros_like(unprocessed_spectrogram)[0:1, :]], axis=0) # Fill last column of freqs with zeros    \n",
    "    audio = stft.invert_spectrogram(unprocessed_spectrogram)\n",
    "    fake_audios[index] = audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_long_audios = fake_audios.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare and save data for the experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import librosa \n",
    "from pydub import AudioSegment\n",
    "\n",
    "from tifresi.phase.modGabPhaseGrad import modgabphasegrad\n",
    "from tifresi.phase.pghi_masked import pghi\n",
    "from tifresi.stft import GaussTruncTF, GaussTF\n",
    "\n",
    "def match_target_amplitude(sound, target_dBFS=-24):\n",
    "    change_in_dBFS = target_dBFS - sound.dBFS\n",
    "    return sound.apply_gain(change_in_dBFS)\n",
    "\n",
    "def saveAudioForlisteningTest(audio, path):\n",
    "    resampled_sound = librosa.core.resample(audio, args['sampling_rate'], 48000)\n",
    "    shifted_sound = (resampled_sound * (2 ** 31 - 1)).astype(np.int32)\n",
    "    sound = AudioSegment(shifted_sound.tobytes(),\n",
    "                         frame_rate=48000,\n",
    "                         sample_width=4, #  4 bytes, so 32 bit sample\n",
    "                         channels=1)     #  mono\n",
    "    sound = sound.fade_in(500)\n",
    "    sound = sound.fade_out(500)\n",
    "\n",
    "    normalized_sound = match_target_amplitude(sound)\n",
    "    normalized_sound.export(path, format='wav')\n",
    "    \n",
    "\n",
    "hop_size = 256\n",
    "stft_channels = 1024\n",
    "stft = GaussTruncTF(256, 1024)\n",
    "\n",
    "base_folder_name = \"maestro-recordings-length/\"\n",
    "# os.mkdir(base_folder_name)\n",
    "\n",
    "real_folder = base_folder_name + 'real/'\n",
    "fake_medium_folder = base_folder_name + \"GAN/\"\n",
    "fake_short_folder = base_folder_name + \"GAN-short/\"\n",
    "fake_long_folder = base_folder_name + \"GAN-long/\"\n",
    "clicked_folder = base_folder_name + \"clicked/\"\n",
    "pghi_folder = base_folder_name + \"pghi/\"\n",
    "\n",
    "# os.mkdir(real_folder)\n",
    "# os.mkdir(fake_medium_folder)\n",
    "# os.mkdir(fake_short_folder)\n",
    "# os.mkdir(fake_long_folder)\n",
    "# os.mkdir(clicked_folder)\n",
    "# os.mkdir(pghi_folder)\n",
    "\n",
    "for index, real_audio in enumerate(real_audios): \n",
    "    start = 480*256 - int((np.random.rand()*2+2) * 22050)\n",
    "    end = 544*256 + int((np.random.rand()*2+2) * 22050)\n",
    "    \n",
    "    #Real\n",
    "    real_start = start+512*256\n",
    "    real_end = end + 512*256\n",
    "    \n",
    "    real = real_audio[real_start:real_end].copy()    \n",
    "    saveAudioForlisteningTest(real, real_folder + str(index) + '.wav')\n",
    "    \n",
    "    #Fake\n",
    "    saveAudioForlisteningTest(fake_medium_audios[index][start:end], fake_medium_folder + str(index) + '.wav')\n",
    "    \n",
    "    #Fake-short\n",
    "    short_start = np.max([start - 256 * 256, 0])\n",
    "    short_end = end - 256 * 256\n",
    "    saveAudioForlisteningTest(fake_short_audios[index][short_start:short_end], fake_short_folder + str(index) + '.wav')\n",
    "    \n",
    "    #Fake-long\n",
    "    long_start = start + 512 * 256\n",
    "    long_end = end + 512 * 256\n",
    "    saveAudioForlisteningTest(fake_long_audios[index][long_start:long_end], fake_long_folder + str(index) + '.wav')\n",
    "   \n",
    "    #Clicked\n",
    "    audio_to_save = real_audio.copy()\n",
    "    audio_to_save = audio_to_save / np.max(np.abs(audio_to_save)) / 2\n",
    "    audio_to_save[(512+480)*256:(512+482)*256] += np.sin(2*np.pi*440*np.arange(0,2*256)/22050) / 2\n",
    "    audio_to_save = audio_to_save[real_start:real_end]\n",
    "\n",
    "    saveAudioForlisteningTest(audio_to_save, clicked_folder + str(index) + '.wav')\n",
    "\n",
    "    \n",
    "    #PGHI\n",
    "    dgt = stft.dgt(real_audio.copy())\n",
    "    spectrogram = np.abs(dgt)\n",
    "    mask = np.ones_like(spectrogram)\n",
    "    mask[:, 512+480:512+480+64] = 0\n",
    "    \n",
    "    tfr = hop_size * stft_channels / len(real_audio)\n",
    "    g_analysis = {'name': 'gauss', 'tfr': tfr}\n",
    "    \n",
    "    tgrad, fgrad = modgabphasegrad('abs', spectrogram, g_analysis, hop_size,\n",
    "                                       stft_channels)\n",
    "    \n",
    "    phase, _ = pghi(spectrogram, tgrad, fgrad, hop_size, stft_channels, len(real_audio), mask, phase=np.angle(dgt))\n",
    "    \n",
    "    reComplexStft = spectrogram * np.exp(1.0j * phase)\n",
    "\n",
    "    audio = stft.idgt(reComplexStft, hop_size, stft_channels)\n",
    "\n",
    "    saveAudioForlisteningTest(audio[real_start:real_end], pghi_folder + str(index) + '.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_short_audios[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(short_start, short_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "120667/22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    start = 480*256 - int((np.random.rand()*2+2) * 22050)\n",
    "    end = 544*256 + int((np.random.rand()*2+2) * 22050)\n",
    "    short_start = start - 256 * 256\n",
    "    short_end = end - 256 * 256\n",
    "\n",
    "    print(short_start/22050)\n",
    "    print(short_end/22050)\n",
    "    print('lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
